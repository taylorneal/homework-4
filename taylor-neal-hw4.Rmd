---
title: "ECO 395 Homework 4: Taylor Neal"
output: rmarkdown::github_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
library(LICORS)
library(foreach)
library(arules)
library(arulesViz)
library(igraph)
library(corrplot)
library(knitr)

wine <- read.csv("https://raw.githubusercontent.com/taylorneal/homework-4/master/data/wine.csv", header = TRUE)
social_marketing <- read.csv("https://raw.githubusercontent.com/taylorneal/homework-4/master/data/social_marketing.csv", header = TRUE)
groceries <- readLines("https://raw.githubusercontent.com/taylorneal/homework-4/master/data/groceries.txt")

set.seed(12)
```
## 1) Clustering and PCA


In this exercise, we will run both PCA and K-means++ clustering on the 11 chemical properties (scaled and centered) for the provided wine data. Our goal is to determine whether either of these unsupervised learning techniques easily distinguish between red and white wines (and further to see if these unsupervised techniques can distinguish between high and low quality wines). We will begin by briefly examining the PCA results, but (in the interest of relative ease of interpretation) clustering results will be our primary focus.

```{r PCA-table, echo = FALSE, fig.align = 'center'}

X = wine[,(1:11)]
X = scale(X, center = TRUE, scale = TRUE)

PCA = prcomp(X, rank = 11)

kable(round(PCA$rotation[,1:5],2))

```

The table above displays the resulting first five principle components for our 11 chemical properties of wine. The downside of utilizing PCA for this analysis is the lack of interpretability of the PCA coefficients in the above table. With a much more extensive knowledge of wine chemical characteristics, we might be able to interpret the loadings of each component. But, as it stands, the PCA1 loading sulfer dioxide, residual sugar and citric acid levels against all else is not very enlightening. However, we will see in a later plot that PCA1 is picking up on red vs white wine in our data (without having to utilize the red/white distinction for learning purposes).

```{r tot-sulf-diox-alc-1, echo = FALSE, fig.align = 'center'}

clust1 = kmeanspp(X, k = 3, nstart = 10)

ggplot(wine) + 
  geom_point(aes(alcohol, total.sulfur.dioxide, 
                 fill = factor(clust1$cluster), color = color), size = 2, pch = 21) + 
  scale_fill_manual(values=c("blue", "cyan", "red")) #+ facet_wrap(~quality)

```

The figure above shows a scatter plot of alcohol percentage vs total sulfur dioxide. These variables were chosen because the scatter plot does a good job of visually separating the red and white wine clusters in our data. K-means++ was run with k equal to three (three was chosen based on maximizing a CH index across a grid of potential k). The color of the points is determined based on resulting cluster. The outline of each point is colored based on the actual color of the wine. We find that one cluster generally identifies red wine and two clusters identify white wines. And, while there are a few miscategorized wines, the clusters appear to do an excellent job of distinguishing between red and white wines.


```{r PCA-scatter, echo = FALSE, fig.align = 'center'}

ggplot(wine) + 
  geom_point(aes(alcohol, total.sulfur.dioxide, 
                 color = PCA$x[,1]), size = 2) +
  scale_color_gradient2(low = 'red', mid = 'light grey', high = 'cyan', 
                        name = 'PC1')

```

Touching back on PCA, the above scatter plot colored by PCA1 for the same variables also appears to accurately pick up on the red vs white wine distinction.

```{r tot-sulf-diox-alc-2, echo = FALSE, fig.align = 'center'}

wine$predict_rw = 'Correct'
wine[clust1$cluster == 3 & wine$color == 'white', 'predict_rw'] = 
  'Incorrect (white)'
wine[clust1$cluster != 3 & wine$color == 'red', 'predict_rw'] = 
  'Incorrect (red)'
white_pct = 1 - sum(wine$predict_rw == 'Incorrect (white)') / 
  sum(wine$color == 'white')
red_pct = 1 - sum(wine$predict_rw == 'Incorrect (red)') / 
  sum(wine$color == 'red')

ggplot(wine) + 
  geom_point(aes(alcohol, total.sulfur.dioxide, 
                 color = factor(predict_rw)), size = 2) +
  scale_color_manual(values=c("light gray", "red", "cyan"))

```


The above scatter plot colors only the wines that our K-means++ algorithm incorrectly categorizes wine color for. Here we can clearly see that our clustering methodology is easily classifying most wines in our data. White wines (of which there are approximately 4,900 in our data) are classified correctly 98.9% of the time. And red wines (of which there are approximately 1,600 in our data) are classified correctly 96.2% of the time.

```{r tot-sulf-diox-alc-3, echo = FALSE, fig.align = 'center'}

ggplot(wine) + 
  geom_point(aes(alcohol, total.sulfur.dioxide, 
                 fill = factor(clust1$cluster), color = color), size = 2, pch = 21) + 
  scale_fill_manual(values=c("blue", "cyan", "red")) + facet_wrap(~quality)

```

Faceting our original scatter plot by wine quality score allows us to see our clusters across each level of quality. We do not find that the clustering algorithm has naturally picked up on distinguishing between wines of varying quality. In order to do this, it is likely we would need to incorporate supervised learning techniques to allow for some kind of determination of the chemical properties that lead to higher quality scores.

## 2) Market Segmentation

Here we are interested in market segments within this social media audience. First, we will seek to identify meaningful market segments by analyzing correlated tweet interests. Then we will utilize K-means++ clustering to demonstrate that it is straightforward to identify individuals as members of said market segments. Based on these results, a discussion of the insights gleaned will follow (with a focus on what actions NutrientH20 can consider to best position its brand with regard to its Twitter audience). Note that of the 36 tweet categories, two ("chatter and "uncategorized") were excluded from the following analysis due to the lack of meaningful information conveyed by those categories.

```{r social-marketing-cor, echo = FALSE, fig.align = 'center', fig.width = 8, fig.height = 8}

SM = social_marketing[,c(3:5,7:37)]

cor_SM = cor(SM)

corrplot(cor_SM, type = "upper", order = "hclust")

```

The correlation plot above has been reordered by clusters in order to more easily identify groups of correlated interests. We find 5 clusters of categories where there are at least three categories highly correlated with each other. These include: (1) Health/Nutrition, Personal Fitness and Outdoors; (2) Cooking, Beauty and Fashion; (3) Politics, Travel and Computers (I would group News with these as well given its noteworthy correlation with each of these categories); (4) College/University, Online Gaming and Sports Playing; (5) Family, Parenting, School, Religion and Sports Fandom. We will discuss each of these categories in more detail later, but next we will use K-means++ clustering with k equal to 6 to see if we can adequately categorize individual users into our market segments as specified. The following matrix heat maps will show us which are the most active categories for each of our determined clusters. 
 
```{r heat-matrix-1, echo = FALSE, fig.align = 'center', fig.width = 6, fig.height = 7}

cluster1 = kmeanspp(SM, 6, nstart = 10)

heatmap(as.matrix(SM[cluster1$cluster == 1,]), Colv = NA, Rowv = NA, main = 
          'Cluster 1 (Health/Nutrition and Fitness)')

```


The above heat map finds that cluster 1 appears to have identified the Health/Nutrition, Personal Fitness and Outdoors category. We note that although "Outdoors" is also a highly correlated interest. The magnitude of the number of Health/Nutrition and Personal Fitness posts is much larger. In the remaining cluster heat maps to follow, we will see that a catch-all other category picks up on a relatively large number of photo sharing oriented twitter users, and that the remaining 4 market segments are also identified utilizing this methodology.

```{r heat-matrix-2, echo = FALSE, fig.width = 6, fig.height = 7, fig.align = 'center'}

heatmap(as.matrix(SM[cluster1$cluster == 2,]), Colv = NA, Rowv = NA, main = 
          'Cluster 2 (Other)')

```

```{r heat-matrix-3, echo = FALSE, fig.width = 6, fig.height = 7, fig.align = 'center'}

heatmap(as.matrix(SM[cluster1$cluster == 3,]), Colv = NA, Rowv = NA, main = 
          'Cluster 3 (Cooking and Fashion/Beauty)')

```

```{r heat-matrix-4, echo = FALSE, fig.width = 6, fig.height = 7, fig.align = 'center'}

heatmap(as.matrix(SM[cluster1$cluster == 4,]), Colv = NA, Rowv = NA, main = 
          'Cluster 4 (Travel, News and Politics)')

```

```{r heat-matrix-5, echo = FALSE, fig.width = 6, fig.height = 7, fig.align = 'center'}

heatmap(as.matrix(SM[cluster1$cluster == 5,]), Colv = NA, Rowv = NA, main = 
          'Cluster 5 (College and Gaming)')

```

```{r heat-matrix-6, echo = FALSE, fig.width = 6, fig.height = 7, fig.align = 'center'}

heatmap(as.matrix(SM[cluster1$cluster == 6,]), Colv = NA, Rowv = NA, main = 
          'Cluster 6 (Family Topics and Sport Fandom)')

```

Based on the above analysis, we see that at least five easily identifiable market segments exist within the Twitter audience of NutrientH20 and that these segments can be relatively easily identified with an unsupervised learning algorithm. Thus, each of these groups can be targeted in distinct ways to increase brand engagement. 

Regarding the Health/Nutrition, Personal Fitness and Outdoors segment, we find that although "Outdoors" is a highly correlated category in this group, Health/Nutrition and Fitness are the primary interests by volume of posts. Thus, posts seeking to engage based on those subjects are likely best for this particular market segment.

Regarding the Cooking, Beauty and Fashion segment, we note that photo sharing is also a highly populated column in our cluster heat map. Perhaps there is merit to the strategy of targeting this audience with photo oriented posts given their high relative interest in aesthetics. 

Regarding the Politics, Travel and Computers segment, these Twitter users seem to be more in tune with news, travel and issues of interest in the political sphere. Posts that have grounding in current events of the world and political issues seem most likely to engage this market segment. 

Regarding the College/University, Online Gaming and Sports Playing segment, we find that online gaming and College/University categories seem to be far more populated in our heat map. A potential course to engage this segment might be through endorsement deals with gamers or through other online gaming content. College sports and relevant endorsement content might be another way to engage this segment. 

Finally, the family topics ("Family", "Parenting", "Religion", "School") and Sports Fandom segment, provides an opportunity to engage with users who are presumably the parents of family units. Thus, family or youth sports oriented content could be an effective way to engage this group. Additionally, this segment probably has the most to gain from leveraging sports endorsement deals as engagement leverage given the high number of tweets related to sports fandom (likely related to specific teams). Determining which teams members of this market segment follow and post about could be of additional interest when thinking about long term engagment.

## 3) Association Rules for Grocery Purchases


In this exercise, we seek to use association rule mining to discover interesting rules related to grocery shopping baskets. Additionally, we will seek to select a subset of rules that can be visualized and sense checked. 

```{r rules-scatter, echo = FALSE, fig.align = 'center', results = 'hide'}

groceries = split(groceries, seq_len(length(groceries)))

for (i in seq(length(groceries))) {
  groceries[[i]] = scan(text = groceries[[i]], what = '', sep = ',', 
                        quiet = TRUE)
}

groceries = lapply(groceries, unique)

groceries = as(groceries, "transactions")

rules = apriori(groceries, parameter = list(support = 0.01, 
                                            confidence = 0.1, maxlen = 2))

plot(subset(rules, lift > 1.0001), measure = c("support", "lift"), shading = "confidence")

```


The figure above displays a scatter plot of associated rules plotted by support and lift (and colored by confidence). Note that determined rules with a lift of 1 were excluded from this plot (said rules have very high confidence and disrupt the color scale while being relatively uninteresting for our purposes). Based on this plot, our thresholds for lift and confidence were chosen as 1.8 and 0.3, respectively. Limiting our rules to those with confidence of at least 0.3 limits us to the top half of our confidence level distribution. And limiting lift to 1.8 gives us enough rules to show an interesting graph while only including those with relatively high lift. 

```{r rules-graph, echo = FALSE, fig.align = 'center', fig.width = 10, fig.height = 8}

sub1 = subset(rules, lift > 1.8 & confidence > 0.3)

plot(sub1, method = 'graph')

```


The resulting graph displays the rule relationships as determined above. We find that "other vegetables" serve as a central element where many items (such as hamburger meat, chicken, onions, etc.) indicate an increased chance of including "other vegetables" in a shopping basket. This makes sense because many of these items would seem to indicate that an individual is planning to cook a full meal where the catch-all of "other vegetables" would commonly capture something in the overarching meal ingredients. The dairy side of our resulting graph is also of interest. We see that berries, cream cheese or curd all provide lift for yogurt also appearing in a basket. Note that by far our largest lift in this graph belongs to the beef to root vegetables rule. This not only makes sense but emphasizes that steak and potatoes remain a very strong component of many red-blooded american diets. 